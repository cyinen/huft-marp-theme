---
marp: true
---

<!--
paginate: true
size: 16:9
theme: am_red

# footer: 我是页脚
header: " "
class: lead

-->

<!-- _class: cover_e -->
<!-- _paginate: "" -->
<!-- _footer: 厚德、笃学、崇实、尚新-->
<!-- _header: "" -->



# MmAP : Multi-Modal Alignment Prompt for Cross-Domain Multi-Task Learning
###### 
AAAI2024
陈银
2024年5月30日

---

![image.png](https://raw.githubusercontent.com/cyinen/imge/master/20240531152903.png)

---


<!-- 
_header: 目录<br>CONTENTS<br>
-->


<!-- _class: toc_b -->
<!-- _footer: "" -->
<!-- _paginate: "" -->

- [Introduction](#4)
- [Method](#5)
- [Experiments](#12)
- [Reference](#18)

---

<!-- _header: Introduction -->

## Motivation

- 解决视觉多任务学习中的挑战，特别是使用预训练视觉语言模型（如CLIP）进行多任务学习时的参数高效调整问题。
- 现有方法集中在单任务适应，且需更新大量参数，导致计算和存储开销大。
- 提出多模态对齐提示（MmAP），通过生成文本提示和视觉提示实现两种模态的对齐​​。

## Main Contributions

- 提出多模态对齐提示（MmAP），有利于CLIP的视觉-语言表示对齐
- 设计用于跨领域图像识别的多任务提示学习框架，包括组共享的MmAP和任务特定的MmAP
- 开发基于CLIP的统一库，用于基准测试各种参数高效调整方法在多任务图像识别中的表现 
- 实验结果表明，本方法使用仅约0.09%的CLIP参数，在两个常用视觉多任务数据集上取得与多任务全微调方法相当的性能

---

<!-- _header: Methods -->

先介绍CLIP的对比学习，再介绍本文的MmAP

#### Contrastive Language-Image Pre-training

视觉编码：
$$[c_{k+1},E_{k+1}]=\mathcal{V}_{k+1}([c_k,E_k])\quad k=0,1,...,\mathcal{K}-1.$$

$$x=\operatorname{ImageProj}(c_{\mathcal{K}}).$$
文本编码：

$$[W_{k+1}]=\mathcal{L}_{k+1}(W_k)\quad k=0,1,...,\mathcal{K}-1.$$
$$z=\text{TextProj}(W_{\mathcal{K}}).$$
零样本预测：$\hat{y}$ is the predict label for the given image
$$p(\hat{y}|x)=\frac{\exp\left(\sin\left(x,z_{{\hat{y}}}\right)/\tau\right)}{\sum_{i=1}^{C}\exp\left(\sin\left(x,z_{i}\right)/\tau\right)},$$

--- 

<!-- _header: Methods -->

![image.png](https://raw.githubusercontent.com/cyinen/imge/master/20240509200222.png)

---

<!-- _header: Methods -->

## Multi-modal Alignment Prompt

已经有很多工作对单个模态设计Prompt, 如VPT, CoOp, 而MaPLe提出的最并发的方法是通过具有相当大参数的MLP使用文本提示生成视觉提示，这在视觉模态和模型效率方面表现出局限性。于是本文提出了we propose Multi-modal Alignment Prompt (MmAP) 同时生成图像$P_{v}\in R^{b\times d_{v}}$和文本的$P_{l}\in R^{b\times d_{l}}$。
首先初始化$P_{s}  \in R^{m \times n}$ 和两个独立的缩放矩阵$\mathcal{M}_l\in R^{\frac bm\times\frac{d_l}n}\mathrm{}\mathcal{M}_v\in R^{\frac bm\times\frac{d_v}n}$
然后使用Kronecker Product (**克罗内克积**)为文本和图像生成对应的prompts:
$$\left.P_{l}=\mathcal{M}_{l}\otimes P_{s}=\left(\begin{array}{ccc}\mathcal{M}_{l_{11}}P_{s}&\cdots&\mathcal{M}_{l_{1n}}P_{s}\\\vdots&\ddots&\vdots\\\mathcal{M}_{l_{m1}}P_{s}&\cdots&\mathcal{M}_{l_{mn}}P_{s}\end{array}\right.\right)$$
$$\\P_{v}=\mathcal{M}_{v}\otimes P_{s}=\left(\begin{array}{ccc}\mathcal{M}_{v_{11}}P_{s}&\cdots&\mathcal{M}_{v_{1n}}P_{s}\\\vdots&\ddots&\vdots\\\mathcal{M}_{v_{m1}}P_{s}&\cdots&\mathcal{M}_{v_{mn}}P_{s}\end{array}\right).$$

---

<!-- _header: Methods -->

>[!note]
>数学上，**克罗内克积**（英语：Kronecker product）是两个任意大小的矩阵间的运算，表示为⊗。简单地说，就是将前一个矩阵的每个元素乘上后一个完整的矩阵。克罗内克积是[外积](https://zh.wikipedia.org/wiki/%E5%A4%96%E7%A7%AF "外积")从向量到矩阵的推广，也是[张量积](https://zh.wikipedia.org/wiki/%E5%BC%A0%E9%87%8F%E7%A7%AF "张量积")在标准基下的矩阵表示。


使用**克罗内克积**方法有两个好处：
1、使用Kronecker Product最大化保持了原来的信息$P_s$ , 有利于两个模态之间的对齐
2、可学习的参数大大减少，从$\mathcal{K}(d_l+d_v)\mathrm{~to~}mn+\frac{\mathcal{K}(d_l+d_v)}{mn},$也减少了过拟合的风险

---

<!-- _header: Methods -->

## Multi-Task Prompt Learning Framework 

<!-- _class: cols-2-64 -->

<div class=limg>

![#c](https://raw.githubusercontent.com/cyinen/imge/master/20240509200653.png)
</div>

<div class=rdiv>

通常，任务相似程度可以通过评估任务之间的渐变冲突来量化。
根据任务的梯度冲突或者任务的相似性，把任务分组，每一个组会共享一个MmAP, 每个任务也有独立的MmAP。

---

<!-- _header: Methods -->

## Multi-Task Prompt Learning Framework 

#### Task Grouping
有研究指出，任务的梯度余弦相似性可以量化两个任务之间的相似性，因此本文通过计算共享参数上的梯度来计算两个任务的相似性。

所有任务共享一个MmAP $P_{glp}$, 那么任务 $i$ 和$j$之间的相似性可以通过下面的方式得到：

$$sim(\mathcal{T}_i,\mathcal{T}_j)=\nabla_{P_{glb}}L_{\mathcal{T}_i}\left(P_{glb}\right)\cdot\nabla_{P_{glb}}L_{\mathcal{T}_j}\left(P_{glb}\right)$$

$L_{\mathcal{T}_i}$ 表示任务$\mathcal{T}$的损失函数，只有当$sim(\mathcal{T}_i,\mathcal{T}_j)>0$, 两个人物之间才是具有互助效应。此外，为了进行稳健的估计，我们在全局共享 MmAP 的训练过程中提供了多个相似性的“快照”。在高层次上，我们同时训练所有任务，在整个训练过程中评估成对任务的相似性，并确定最大化任务间总相似性的任务组。（描述的不太懂）

--- 

<!-- _header: Method -->
<br>

#### Multi-Task Prompt Learning

给定N个下游任务 $\{T_i\}^N_{i=1}$，根据梯度相似性将它们划分为几个不相交的组。
- $G$：表示一个由$|G|$个任务组成的任务组（$1 \leq |G| \leq N$）。

- **组共享MmAP**： 
	- 源提示：$P_G = \{P^k_G\}^K_{k=1}$，
	- 语言分支缩放矩阵：$M_{G_l} = \{M^k_{G_l}\}^K_{k=1}$
	- 视觉分支缩放矩阵：$M_{G_v} = \{M^k_{G_v}\}^K_{k=1}$。
	
组共享的MmAP由组$G$内的所有任务累积更新，实现了相似任务之间的互补收益。

- 此外，对于组$G$中的每个任务，我们构建任务特定的MmAP以学习独特的任务特征：
- **Task-specific MmAP**
	- 包括源提示：$P_T = \{P^k_T\}^K_{k=1}$，
	- 语言分支的缩放矩阵 $M_{T_l} = \{M^k_{T_l}\}^K_{k=1}$
	- 视觉分支缩放矩阵：$M_{T_v} = \{M^k_{T_v}\}^K_{k=1}$。

---

<!-- _header: Method -->


在训练组$G$中的某个任务$T$期间，我们首先生成两个编码器中第$k$层的文本和图像提示，然后通过组合类标记、生成的提示和上一层的文本/图像标记重构输入标记。

因此，文本和图像编码器中第$k$层的计算可以正式表示为：

$$
\begin{align*}
[-, -, W_k] &= L_k ([P^k_{G_l}, P^k_{T_l}, W_{k−1}]) \\
            &= L_k ([P^k_G \otimes M^k_{G_l}, P^k_T \otimes M^k_{T_l}, W_{k−1}])
\end{align*}
$$


$$
\begin{align*}
[c_k, -, -, E_k] &= V_k ([c_{k−1}, P^k_{G_v}, P^k_{T_v}, E_{k−1}]) \\
                 &= V_k ([c_{k−1}, P^k_G \otimes M^k_{G_v}, P^k_T \otimes M^k_{T_v}, E_{k−1}])
\end{align*}
$$


这里 $[·, ·]$ 表示连接操作。最后，通过优化以下损失来累积更新组共享MmAP：

$$
L(P_G, M_{G_l}, M_{G_v}) = \sum_{T \in G} L_T (P_G, M_{G_l}, M_{G_v})
$$

任务特定的MmAP通过以下方式训练：

$$
L(P_T, M_{T_l}, M_{T_v}) = L_T (P_T, M_{T_l}, M_{T_v})
$$

其中 $L_T$ 是任务$T$的交叉熵损失。

---

<!-- _header: Experiments -->

- 数据集：在**Office-Home**和**MiniDomainNet**实验

	- **Office-Home** contains images from four tasks: ==*Art, Clipart, Product and Real World*.== Each task covers images from 65 object categories collected under office and home settings. There are about 15,500 images in total.
	- **MiniDomainNet** takes a subset of DomainNet, which is an extremely challenging dataset for multi-task learning. MiniDomainNet has 140,000 images distributed among *126 categories*. It contains four different tasks: Clipart, Painting, Sketch and Real.
- 都是分类任务，图像风格不同

---

<!-- _header: Experiments -->
<br>

## Compare With SOTA Methods


<!-- _class: cols-2-64 -->

<div class=limg>

![#c](https://raw.githubusercontent.com/cyinen/imge/master/20240530170118.png)
</div>

<div class=rdiv>

实验表明，作者提出的方法效果最好，微调的参数基本上也很少（第二），实现了 性能和参数上的平衡。

</div>

---

<!-- _header: Experiments -->
<br>

##  Ablation Study

<!-- _class: cols-2-64 -->

<div class=limg>

![#c](https://raw.githubusercontent.com/cyinen/imge/master/20240509201144.png)
</div>

<div class=rdiv>

对task- specific MmAP 和 group-shared MmAP 消融实验证明方法确实有效

</div>

---
<!-- _header: Experiments -->
<br>

##  Ablation Study

<!-- _class: cols-2-64 -->

<div class=limg>

![#c](https://raw.githubusercontent.com/cyinen/imge/master/20240530183640.png)
</div>

<div class=rdiv>

使用简单的MLP连接两个模态同样有效, 提出的方法比MLP更好，参数更少

</div>

---

<!-- _header: Experiments -->
##  Ablation Study

<!-- _class: rows-2-73 -->

<div class=limg>

![#c](https://raw.githubusercontent.com/cyinen/imge/master/20240530170842.png)
</div>

<div class=rdiv>
不同的Zero-shot数据量的影响，训练数据越多效果越好。但是相比与Zero-shot方法， 别的变体都差，是因为额外的参数破坏了原有的结构，并且没有微调。

</div>

---

<!-- _header: Reference -->

- [1] Xin, Yi, Junlong Du, Qiang Wang, Ke Yan和Shouhong Ding. 《MmAP: Multi-Modal Alignment Prompt for Cross-Domain Multi-Task Learning》. Proceedings of the AAAI Conference on Artificial Intelligence 38, 期 14 (2024年3月24日): 16076–84. https://doi.org/10.1609/aaai.v38i14.29540.



---

<!-- _class: lastpage -->
<!-- _footer: "" -->

###### Thank you for your attention! 

<div class="icons" >

- <i class="fa-solid fa-envelope"></i>
  - 邮箱：cyinen@qq.com

<div>

